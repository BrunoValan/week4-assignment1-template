{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Assignment \n",
    "Use this notebook to complete week 4 assignment of the Python Bootcamp for Data course. Each question is described with a Markdown cell explaining the requirements and expectations. Update the corresponding cells and as always, wait until the autograding completes on your GitHub repository \n",
    "\n",
    "The variables in the question cells are pre-assigned to `None` so that you can reuse them. The autograding tests uses those variables names to ensure the expected values are present. Do not change the variable names, otherwise a `NameError` will show in the autograding test logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Load the `city_census.csv` dataset in this repo, assign the dataframe to `df1` and manipulate the dataframe so that:\n",
    "- Only 15 items are present\n",
    "- Exports to a Python dictionary\n",
    "\n",
    "Assign the Python dictionary with the 15 items from the dataframe to `df1_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"city_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Load the `city_census.csv` dataset again. There are some rows where the `weight` value is `NaN`. Drop those rows from the `dataframe` and assign the resulting value to the `df2` variable. \n",
    "Hint: Make sure that the dropped rows are reflected in the dataframe, otherwise the test will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"city_census.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Load the `city_census.csv` dataset again. Manipulate the `dataframe` so that only entries from North Carolina are present. Assign the resulting dataframe to the `df3` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"city_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Load the `city_census.csv` dataset again. Manipulate the `dataframe` so that only 20 entries with persons that are over 150 in weight are used. Assign the resulting `dataframe` to the `df4` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"city_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Load the `city_census.csv` dataset again. Manipulate the `dataframe` so that it holds anyone that is over 200 in weight from the state of California. Assign the resulting `dataframe` to the `df5` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv(\"city_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Manipulate the `dataframe` so that it holds anyone that has a first name that includes the (exact) `\"al\"` letters. Assign the resulting operation to the `df6` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv(\"city_census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Create a function to manipulate the `dataframe` so that it updates any `NaN` value from the `weight` columnn to 0. Assign the resulting operation on the `dataframe` to the `df7` variable.\n",
    "\n",
    "Hint: Use `math.isnan()` to check if a value is `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Hint: use math.isnan function to check if a value is NaN when you create your\n",
    "# helper function\n",
    "\n",
    "df7 = pd.read_csv(\"city_census.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Manipulate the `dataframe` so that it has a new column called `\"health_issue\"`. This column should have only `1` or `0`. If the person is above `240` in weight, this column should have a `1`. Otherwise a `0`. Assign the resulting operation on the `dataframe` to the `df8` variable.\n",
    "\n",
    "Hint: first create the `health_issue` column with the matching weight and then use a function to transform the boolean into a 1 or a 0. Don't forget to assign values to new columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>age</th>\n",
       "      <th>state</th>\n",
       "      <th>weight</th>\n",
       "      <th>health_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Earl</td>\n",
       "      <td>Myrick</td>\n",
       "      <td>36</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vikki</td>\n",
       "      <td>Gallegos</td>\n",
       "      <td>31</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>Binegar</td>\n",
       "      <td>15</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edwina</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>53</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Hinkle</td>\n",
       "      <td>2</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Martha</td>\n",
       "      <td>Hudgins</td>\n",
       "      <td>31</td>\n",
       "      <td>Utah</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Danny</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>55</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beverly</td>\n",
       "      <td>Skinner</td>\n",
       "      <td>67</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Martha</td>\n",
       "      <td>Brown</td>\n",
       "      <td>2</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ofelia</td>\n",
       "      <td>Williams</td>\n",
       "      <td>82</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Christina</td>\n",
       "      <td>Jiggetts</td>\n",
       "      <td>15</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>Sale</td>\n",
       "      <td>35</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Anita</td>\n",
       "      <td>Everhardt</td>\n",
       "      <td>46</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Clinton</td>\n",
       "      <td>Turner</td>\n",
       "      <td>66</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Judy</td>\n",
       "      <td>Gramm</td>\n",
       "      <td>51</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>William</td>\n",
       "      <td>Mccormick</td>\n",
       "      <td>51</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Louise</td>\n",
       "      <td>Magana</td>\n",
       "      <td>19</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>John</td>\n",
       "      <td>Rosales</td>\n",
       "      <td>55</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Christine</td>\n",
       "      <td>Ware</td>\n",
       "      <td>32</td>\n",
       "      <td>Washington</td>\n",
       "      <td>191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Raymond</td>\n",
       "      <td>Poole</td>\n",
       "      <td>2</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lois</td>\n",
       "      <td>Ford</td>\n",
       "      <td>29</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Joyce</td>\n",
       "      <td>Sites</td>\n",
       "      <td>64</td>\n",
       "      <td>Texas</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Dan</td>\n",
       "      <td>Stone</td>\n",
       "      <td>40</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rosie</td>\n",
       "      <td>Doe</td>\n",
       "      <td>52</td>\n",
       "      <td>New York</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>Bogart</td>\n",
       "      <td>12</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Jo</td>\n",
       "      <td>Guevara</td>\n",
       "      <td>5</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>James</td>\n",
       "      <td>Williams</td>\n",
       "      <td>55</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Linda</td>\n",
       "      <td>Stiles</td>\n",
       "      <td>4</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Henry</td>\n",
       "      <td>Silmon</td>\n",
       "      <td>77</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Emma</td>\n",
       "      <td>Mcdonald</td>\n",
       "      <td>7</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Shawn</td>\n",
       "      <td>Anzalone</td>\n",
       "      <td>82</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Ford</td>\n",
       "      <td>89</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>Sam</td>\n",
       "      <td>5</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Kitty</td>\n",
       "      <td>Ford</td>\n",
       "      <td>38</td>\n",
       "      <td>Texas</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Brustmann</td>\n",
       "      <td>77</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>James</td>\n",
       "      <td>Kendall</td>\n",
       "      <td>3</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Edward</td>\n",
       "      <td>Barbe</td>\n",
       "      <td>56</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Holly</td>\n",
       "      <td>Britten</td>\n",
       "      <td>70</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Nick</td>\n",
       "      <td>Kittleson</td>\n",
       "      <td>58</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Edward</td>\n",
       "      <td>Bradfield</td>\n",
       "      <td>52</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        first       last  age          state  weight  health_issue\n",
       "0        Earl     Myrick   36           Iowa   208.0             0\n",
       "1       Vikki   Gallegos   31           Iowa   187.0             0\n",
       "2   Elizabeth    Binegar   15       Nebraska     NaN             0\n",
       "3      Edwina     Garcia   53        Arizona   170.0             0\n",
       "4      Thomas     Hinkle    2        Georgia     NaN             0\n",
       "5      Martha    Hudgins   31           Utah   227.0             0\n",
       "6       Danny     Wilson   55           Iowa   205.0             0\n",
       "7     Beverly    Skinner   67       Missouri   233.0             0\n",
       "8      Martha      Brown    2       Illinois     NaN             0\n",
       "9      Ofelia   Williams   82     New Mexico   192.0             0\n",
       "10  Christina   Jiggetts   15     New Jersey     NaN             0\n",
       "11    Barbara       Sale   35       Virginia   170.0             0\n",
       "12      Anita  Everhardt   46         Oregon   157.0             0\n",
       "13    Clinton     Turner   66        Georgia   208.0             0\n",
       "14       Judy      Gramm   51  West Virginia   217.0             0\n",
       "15    William  Mccormick   51           Iowa   148.0             0\n",
       "16     Louise     Magana   19       Virginia   174.0             0\n",
       "17       John    Rosales   55      Minnesota   165.0             0\n",
       "18  Christine       Ware   32     Washington   191.0             0\n",
       "19    Raymond      Poole    2       Illinois     NaN             0\n",
       "20       Lois       Ford   29        Vermont   189.0             0\n",
       "21      Joyce      Sites   64          Texas   211.0             0\n",
       "22        Dan      Stone   40        Arizona   193.0             0\n",
       "23      Rosie        Doe   52       New York   222.0             0\n",
       "24    Lincoln     Bogart   12       New York     NaN             0\n",
       "25         Jo    Guevara    5        Alabama     NaN             0\n",
       "26      James   Williams   55       Virginia   208.0             0\n",
       "27      Linda     Stiles    4       Kentucky     NaN             0\n",
       "28      Henry     Silmon   77         Oregon   227.0             0\n",
       "29       Emma   Mcdonald    7       Kentucky     NaN             0\n",
       "30      Shawn   Anzalone   82       Colorado   243.0             1\n",
       "31     Thomas       Ford   89        Indiana   142.0             0\n",
       "32    Barbara        Sam    5   Pennsylvania     NaN             0\n",
       "33      Kitty       Ford   38          Texas   211.0             0\n",
       "34     Thomas  Brustmann   77   North Dakota   174.0             0\n",
       "35      James    Kendall    3    Connecticut     NaN             0\n",
       "36     Edward      Barbe   56     New Jersey   207.0             0\n",
       "37      Holly    Britten   70       Delaware   195.0             0\n",
       "38       Nick  Kittleson   58  West Virginia   239.0             0\n",
       "39     Edward  Bradfield   52       Michigan   155.0             0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8 = pd.read_csv(\"city_census.csv\") \n",
    "df8['health_issue'] = df8['weight'] > 240\n",
    "\n",
    "#df8.query(\"health_issue == True\").head()\n",
    "\n",
    "def f(value):\n",
    "    return int(value)\n",
    "\n",
    "\n",
    "df8['health_issue'] = df8['health_issue'].apply(f)\n",
    "df8.head(40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Load the `city_census.csv` dataset again. Manipulate the `dataframe` so that all rows having `\"North Carolina\"`for state, have `N. Carolina` instead. Assign the resulting operation on the `dataframe` to the `df9` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.read_csv(\"city_census.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "From the NumPy array below, split in two equal parts and then reduce the resulting arrays by having only values higher than 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([18, 11, 8, 33, 23, 42, 82, 11, 8, 7, 1, 23])\n",
    "\n",
    "# Assign your splits to these variables, remember to reduce them with values only higher than 15\n",
    "array_split_1 = None\n",
    "array_split_2 = None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('week1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f867051daff54b2be0af78a2590c7d46393a1771e012c742199fe618e8e4c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
